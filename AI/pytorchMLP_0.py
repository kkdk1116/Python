# -*- coding: utf-8 -*-
"""pytorchMLP_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1we4oO6BjAi7o2CjpmnH3K8oc3-WMkfJP
"""

# MLP by pyTorch, CPU version
# 4 class : T/C/E/L
# 3 patterns / 1 class
# 2 Layer

#import numpy as np
import math as m

import torch

#from torch import tensor
#from torchvision import datasets
#from torchvision.transforms import ToTensor
import torch.nn as nn # 신경망

#import torch.nn.functional as F

import torch.optim as optim # 경사하강법 함수 등

lrate = 0.01
INDIM   = 26
H1DIM    = 10
H2DIM    = 10
OUTDIM  = 4

PTTN_NUM    = 12

# 12 by 26 행렬
# x[12][26]

#x  = np.array([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
x  = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-1
               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
                     0.0, 0.0, 1.0, 0.0, 1.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-2
               [1.0, 0.0, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0, 
                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-3
               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-1
               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
                     1.0, 0.0, 0.0, 0.0, 1.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-2
               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 1.0, 
                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #C-3
               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-1
               [1.0, 0.5, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-2
               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     0.5, 1.0, 1.0, 1.0, 1.0 ],  #E-3               
               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #L-1
               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 1.0, 
                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #L-2
               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     1.0, 0.0, 0.0, 0.0, 0.0, 
                     0.3, 1.0, 1.0, 1.0, 1.0 ] ] )  #L-3                            

# 12 by 4 행렬

#t  = np.array([ [1.0, 0.0, 0.0, 0.0],
yt  = torch.tensor([ [1.0, 0.0, 0.0, 0.0],               
                [1.0, 0.0, 0.0, 0.0],
                [1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 0.0, 1.0],
                [0.0, 0.0, 0.0, 1.0],
                [0.0, 0.0, 0.0, 1.0] ])

class network(nn.Module):  # 네트워크 모델 정의
    def __init__(self, inDim, h1Dim, outDim):
        super(network, self).__init__()
        self.fc1 = nn.Linear(inDim, h1Dim)  
        self.fc2 = nn.Linear(h1Dim, outDim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.sigmoid(self.fc1(x))  # 히든 1층의 출력
        x = self.sigmoid(self.fc2(x))  # 출력층의 출력
        return x


model = network(INDIM, H1DIM, OUTDIM)

optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss()


Epochs = 10000

def train():
    for epoch in range(Epochs):
        loss_sum = 0
        for p in range(PTTN_NUM):

            X = x[p]              # 0_1. input()
            Y = yt[p]

            model.zero_grad()     # 0_2. initialize()
            optimizer.zero_grad()

            prediction = model(X)  # 1. forward() 결과 출력

            loss = criterion(prediction, Y.to(torch.float32))  # 2_1. 로스 계산
            loss.backward()  # 2_2. backward(), 로스 역전파

            optimizer.step()  # 3. wgt_upodate(), 웨이트 수정

            loss_sum += loss.item()

        if epoch%1000 == 0 :
            print("epoch:", epoch, "tss: {:.3f}".format(loss_sum))
            test()

def test():
    correct = 0
    # 데이터로더에서 하나씩 꺼내 추론
    with torch.no_grad():
        for p in range(PTTN_NUM):
            X = x[p]
            Y = yt[p]
            outputs = model(X)  # 출력 계산
            print("Target:",Y, "Output:",outputs)


train()
print("End Train !!!")


#x=[12x26]

print(x.shape)
print(yt.shape)


# 추가한 부분

#w=[26x4]     w[26][4]

#x에서 12, w에서 4 답=[12x4]

# w2 b2 해서 2층으로 해보기

# 네트워크 선언
w = torch.randn(INDIM, OUTDIM, requires_grad=True) # requires_grad=True 반드시 필요  
#w2 (h1dim .re)
b = torch.randn(OUTDIM, requires_grad=True)
#b2 (h1, out, re)
z = torch.matmul(x, w)+b


#loss = torch.n.functional.binary_cross_entropy_with_logits(z, yt)
floss = nn.MSELoss()
optimizer = optim.SGD([w, b],lr=0.01)
#optimizer ([w1,b1,w2,b2],lr=lrate])

# 웨이트, 바이너리, 12x4패턴
print(w)
print(b)
print(z)

print(x.shape, w.shape, z.shape)

"""   # 발 코딩 버전
optimizer.zero_grad()
z = torch.matmul(x, w) +b # 인풋이 들어가면 웨이트와 곱해서 포워딩 한다. x를 w거쳐서 아웃풋 구함
print("z :", z)

loss = floss(z, yt)
#print("before:" , loss)
loss.backward() #에러를 역전파시킨다.
#print("after:" , loss)
optimizer.step() #웨이트(w)와 바이어스(b)를 수정하는 함수. 웨이트 업데이트


optimizer.zero_grad()
z=torch.matmul(x, w) +b
print("z :", z)

loss = floss(z, yt)
#print("before:" , loss)
loss.backward() #에러를 역전파시킨다.
#print("after:" , loss)
optimizer.step() #웨이트(w)와 바이어스(b)를 수정하는 함수. 웨이트 업데이트
"""

sigmoid = nn.Sigmoid() #시그모이드 인스턴스 생성


# 깔끔하게 한 버전

for i in range(20000) :
  optimizer.zero_grad() #초기화
  zt = torch.matmul(x, w)+b # 인풋이 들어가면 웨이트와 곱해서 포워딩 한다. x를 w거쳐서 아웃풋 구함
  z = sigmoid(zt)
  #z1 = torch.matmul(x, w)+b # 인풋이 들어가면 웨이트와 곱해서 포워딩 한다. x를 w거쳐서 아웃풋 구함
  #z1 = sigmoid(zt)
  #z2 = torch.matmul(x, w)+b # 인풋이 들어가면 웨이트와 곱해서 포워딩 한다. x를 w거쳐서 아웃풋 구함
  #z2 = sigmoid(zt)

  if i%1000 == 0 :
      print("epoch:", i, "z:",z)

  loss = floss(z, yt)
  #loss = floss (z2, yt2?)
  loss.backward() #에러역전파
  optimizer.step() #웨이트업데이트

class network(nn.Module): # 네트워크 모델 정의
  def __init__(self,inDim, h1Dim, outDim):  # 생성할 때
    super(network, self).__init__()
    self.fc1 = nn.Linear(inDim, h1Dim) # in_features, out_features, bias=True
    self.fc2 = nn.Linear(h1Dim, outDim) 
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):     # 실행할 때
    x1 = self.sigmoid(self.fc1(x)) # 히든 1층의 출력
    x2 = self.sigmoid(self.fc2(x1)) # 출력층의 출력
    return x2

if torch.cuda.is_available():
  device = torch.device("cuda:0")
  print("running on the GPU")
else:
  device = torch.device("cpu")
  print("running on the CPU")

model = network(INDIM, H1DIM, OUTDIM)
#model.to(device)

Epochs = 3000

optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELOSS()

#optimizer = optim.Adam(params=model.parameters(), lr=0.01)

def train():
  for epoch in range(Epochs):
    loss_sum = 0
    for p in range(PTTN_NUM):
      X = x[p]
      y = yt[p]
      #model.zero_grad()
      optimizer.zero_grad()

      prediction = model(X) # 1, forward